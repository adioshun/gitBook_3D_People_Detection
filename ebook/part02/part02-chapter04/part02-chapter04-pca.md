# 3\_PCA

Principal components are extracted by singular values decomposition on the covariance matrix of the centered input cloud. Available data after pca computation are the mean of the input data, the eigenvalues \(in descending order\) and corresponding eigenvectors. Other methods allow projection in the eigenspace, reconstruction from eigenspace and update of the eigenspace with a new datum \(according Matej Artec, Matjaz Jogan and Ales Leonardis: "Incremental PCA for On-line Visual Learning and Recognition"\).





### 

### 

### 

### [http://www.ctralie.com/Teaching/COMPSCI290/Lectures/10\_PCA/slides.pdf](http://www.ctralie.com/Teaching/COMPSCI290/Lectures/10_PCA/slides.pdf)

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### 

### \#\# 1 공분산 행렬의 고유 벡터 계산 





### 2 공분산 행렬 

![](../../../.gitbook/assets/image%20%281%29.png)



### 3 공분산 

서로 다른 두 확률변수의 상관관계를 나타냄

두 확률변수의 편차의 곱.

![](../../../.gitbook/assets/image%20%282%29.png)



### 4 분산 

즉 모든 x값에 대해 x평균과의 차이를 제곱해서 다 더한 다음에 x의 수로 나누면 평균적인 차이의 제곱, 즉 분산이 나온다.  


![](../../../.gitbook/assets/image%20%283%29.png)

---

![http://www.joon.pe.kr/blog/140](../../../.gitbook/assets/image%20%284%29.png)

